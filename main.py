# -*- coding: utf-8 -*-
"""D2_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16M907mf0tzOXBhAnalqCaQ7TgE64rEeB
"""

from google.colab import files
upload =files.upload()

import pandas as pd
df=pd.read_csv('dataset.csv')
df.head()
#display top section of the data

df.describe()
#describing the data set

import numpy as np
import pandas
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import metrics
from datetime import datetime
from random import sample
from collections import defaultdict
from sklearn.metrics import r2_score
from math import sqrt
from sklearn.metrics import mean_squared_error
from sklearn import tree, datasets, neighbors, svm, preprocessing
from sklearn.datasets import load_iris
from sklearn.ensemble import AdaBoostClassifier
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler, StandardScaler
#importing all librariers

print(df.info())
#displaying all the columns data type ,null or not count

print(df['prefix'].unique())
print(df['prefix'].nunique())
print(df['equinox'].unique())
print(df['equinox'].nunique())

cleandata = df.drop(['id', 'pdes', 'name', 'prefix', 'equinox'], axis='columns', inplace=False)

cleandata["diameter"] = cleandata["diameter"].replace(np.NaN, cleandata["diameter"].mean())
cleandata["albedo"] = cleandata["albedo"].replace(np.NaN, cleandata["albedo"].mean())
cleandata["diameter_sigma"] = cleandata["diameter_sigma"].replace(np.NaN, cleandata["diameter_sigma"].mean())
cleandata["H"] = cleandata["H"].replace(np.NaN, cleandata["H"].mean())

cleandata = cleandata[cleandata['sigma_ad'].notna()]
cleandata = cleandata[cleandata['ma'].notna()]

cleandata.columns

df=cleandata
df['neo'] = df['neo'].astype('category')
df['pha'] = df['pha'].astype('category')
df['class'] = df['class'].astype('category')
df['orbit_id'] = df['orbit_id'].astype('category')

df['orbit_id'].nunique()

df = df.reset_index(drop=True)

print(df.isnull().sum())
# Check for missing values

# Shrinked the dataset to a manageable size
df = df.head(100000)
print('File sample reduced')

print(df['pha'].value_counts(normalize=True))
name = ['Not a Potential Hazard', 'Potential Hazard']
plt.title("Asteroid Potential Hazard")
plt.pie(df['pha'].value_counts(), labels=name, autopct='%0.4f%%', shadow=True, startangle=90)
plt.show()
#Printing the count of potential hazard asteroid

neocount=df['neo'].value_counts(normalize=True)
name = ['Not a Near Earth Object', 'Near Earth Object']
plt.title("Near Earth Objects")
plt.pie(neocount, labels=name, autopct='%0.4f%%', shadow=True, startangle=90)
plt.show()
#printing the count of near earth object

import matplotlib.pyplot as plt
df.plot()
plt.show()
#Printing the shape of the dataset

df.hist(figsize=(30,25))
plt.show()
#Printing the shape of the dataset

df_subset = df[df.columns[~df.columns.isin(['spkid', 'full_name','neo', 'pha', 'orbit_id', 'class'])]]

df.head()

# Calculate correlation matrix
correlation_matrix = df_subset.corr()

# Plot correlation matrix
plt.figure(figsize=(30, 25))
plt.title("Correlation matrix of Potential Hazard Asteroids data")
sns.heatmap(data=correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1, linewidth=1.5, annot=True)
plt.show()

df_subset = df_subset.drop(['epoch_mjd', 'epoch_cal','sigma_w','sigma_a','a'], axis='columns', inplace=False)

df.head()

# Calculate correlation matrix
correlation_matrix_subset = df_subset.corr()

# Plot correlation matrix
plt.figure(figsize=(30, 25))
plt.title("Correlation matrix of Potential Hazard Asteroids data")
sns.heatmap(data=correlation_matrix_subset, cmap='mako', vmin=-1, vmax=1, linewidth=1.5, annot=True)
plt.show()

from sklearn import preprocessing

scaler = preprocessing.MinMaxScaler()
df_scaled = scaler.fit_transform(df_subset)
df_scaled = pd.DataFrame(df_scaled, columns=df_subset.columns)
df = pd.concat([df[['spkid', 'full_name','neo', 'pha', 'orbit_id', 'class']],df_scaled],axis=1)
df_scaled.head()
df.head()
df.describe()

df1 = pd.get_dummies(df, columns=['neo', 'class', 'orbit_id'])
df1.head()

from sklearn.model_selection import train_test_split
X = df1.drop(['spkid', 'full_name', 'pha'], axis=1)
y = df1.iloc[:]['pha']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1,stratify=y)
print("Rows with label 'Y': {}".format(sum(y_train == 'Y')))
print("Rows with label 'N': {}".format(sum(y_train == 'N')))

X.isnull().sum()

from sklearn import metrics  # Import the metrics module

def CalculationOfMetric(y_test, pred):
    precision_metric = metrics.precision_score(y_test, pred, average="macro")
    recall_metric = metrics.recall_score(y_test, pred, average="macro")
    accuracy_metric = metrics.accuracy_score(y_test, pred)
    f1_metric = metrics.f1_score(y_test, pred, average="macro")

    print('Precision metric:', round(precision_metric, 4))
    print('Recall Metric:', round(recall_metric, 4))
    print('Accuracy Metric:', round(accuracy_metric, 4))
    print('F1 score:', round(f1_metric, 4))

from sklearn.linear_model import LogisticRegression

# Instantiate the model
lr = LogisticRegression(max_iter= 10000) # create object for the class

# Fit to train model with features and labels
lr.fit(X_train, y_train)

# Predict for train set
lr_pred = lr.predict(X_train)

# Calculate evaluation metrics for training data
print("Metrics for Training Data:")
CalculationOfMetric(y_train, lr_pred)

# Predict for test set
lr_pred = lr.predict(X_test)

# Calculate evaluation metrics for Testing Data
print("Metrics for Testing Data:")
CalculationOfMetric(y_test, lr_pred)

print(metrics.confusion_matrix(y_test, lr_pred))

from sklearn.ensemble import RandomForestClassifier

# Instantiate model with 150 decision trees
rf = RandomForestClassifier(n_estimators = 150, random_state = 1551)

# Train the model on training data
rf.fit(X_train, y_train)

# Predict for train set
rf_pred = rf.predict(X_train)

# Calculate evaluation metrics for training data
print("Metrics for Training Data:")
CalculationOfMetric(y_train, rf_pred)

# Predict for test set
rf_pred = rf.predict(X_test)

# Calculate evaluation metrics for Testing Data
print("Metrics for Testing Data:")
CalculationOfMetric(y_test, rf_pred)

print(metrics.confusion_matrix(y_test, rf_pred))

from sklearn.neural_network import MLPClassifier


# Instantiate model
mlp = MLPClassifier(hidden_layer_sizes=(500,), max_iter=2000, random_state=1551)

# Train the model on training data
mlp.fit(X_train, y_train)

# Predict for training set
mlp_train_pred = mlp.predict(X_train)

# Calculate evaluation metrics for training data
print("Metrics for Training Data:")
CalculationOfMetric(y_train, mlp_train_pred)

# Predict for test set
mlp_pred = mlp.predict(X_test)

# Calculate evaluation metrics for testing data
print("Metrics for Testing Data:")
CalculationOfMetric(y_test, mlp_pred)

# Print confusion matrix for test data
print("\nConfusion Matrix for Testing Data:")
print(metrics.confusion_matrix(y_test, mlp_pred))

from sklearn.naive_bayes import GaussianNB
nb_classifier = GaussianNB()

params_NB = {'var_smoothing': np.logspace(0,-9, num=20)}
gs_NB = GridSearchCV(estimator=nb_classifier,
                 param_grid=params_NB,
                 verbose=1,
                 scoring='accuracy')
gs_NB.fit(X_train, y_train)
gs_NB.best_params_

gs_NBbest = gs_NB.predict(X_train)
CalculationOfMetric(y_train, gs_NBbest)

gs_NBbest = gs_NB.predict(X_test)
CalculationOfMetric(y_test, gs_NBbest)

print(metrics.confusion_matrix(y_test, gs_NBbest))

import numpy as np
import matplotlib.pyplot as plt

def plot_classification_metrics(models, metrics):
    """
    Plot bar plots for classification metrics of multiple models.

    Parameters:
    - models (list of str): List of model names.
    - metrics (dict): Dictionary containing metrics for each model.
                      Keys are metric names and values are lists of metric values
                      corresponding to each model.

    Returns:
    - None (Displays the bar plots).
    """

    num_models = len(models)
    num_metrics = len(metrics)
    bar_width = 0.2
    colors = ['blue', 'orange', 'green', 'red']

    # Set up figure and axis
    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(num_metrics * 5, 6))

    # Iterate over each metric
    for i, (metric_name, metric_values) in enumerate(metrics.items()):
        ax = axes[i] if num_metrics > 1 else axes

        # Plot metric values for each model
        ind = np.arange(num_models)
        ax.bar(ind, metric_values, width=bar_width, color=colors[i])

        # Set labels and title
        ax.set_ylabel(metric_name)
        ax.set_title(f'{metric_name} for Each Model')
        ax.set_xticks(ind)
        ax.set_xticklabels(models, rotation=45, ha='right')

    # Adjust layout and display the plots
    plt.tight_layout()
    plt.show()


# Example usage
models = ['Logistic Regression', 'Random Forest', 'MLP', 'Gaussian Naive Bayes']
metrics = {
    'Precision': [0.7831, 0.9999, 0.7831, 0.5959],
    'Recall': [0.7654, 0.9844, 0.7654, 0.7333],
    'Accuracy': [0.9991, 0.9999, 0.9991, 0.9973],
    'F1 score': [0.774, 0.9921, 0.774, 0.6357]
}

plot_classification_metrics(models, metrics)